{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get MAPE\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get MAE, MAPE, MASE\n",
    "# if we do lag 1-7, there are NA generated for the first 7 days_prior, so we need to exclude them\n",
    "def eval2(df):\n",
    "    df['fcst_svv']=df['OTB']-df['OTB']*df['predict_cxl_rate']\n",
    "    df=df[df['days_prior']!=0]\n",
    "    avg_svv_rt = 0.9239133\n",
    "    df['abs_diff_1']=abs(df['OTB_to_survive']-df['fcst_svv'])\n",
    "    df['abs_diff_2']=abs(df['OTB_to_survive']-avg_svv_rt*df['OTB'])\n",
    "    #df1=df[df['dp_range']=='1_7']\n",
    "    df2=df[df['dp_range']=='8_14']\n",
    "    df3=df[df['dp_range']=='15_21']\n",
    "    df4=df[df['dp_range']=='22_28']\n",
    "    df5=df[df['dp_range']=='29_32']\n",
    "    # MAE\n",
    "    #mae1=mean_absolute_error(df1['OTB_to_survive'], df1['fcst_svv'])\n",
    "    mae2=mean_absolute_error(df2['OTB_to_survive'], df2['fcst_svv'])\n",
    "    mae3=mean_absolute_error(df3['OTB_to_survive'], df3['fcst_svv'])\n",
    "    mae4=mean_absolute_error(df4['OTB_to_survive'], df4['fcst_svv'])\n",
    "    mae5=mean_absolute_error(df5['OTB_to_survive'], df5['fcst_svv'])\n",
    "    maedf = pd.DataFrame({'dp_range' : ['8_14','15_21','22_28','29_32'],\n",
    "                           'MAE':[mae2, mae3, mae4, mae5]})\n",
    "    #MASE\n",
    "    #mase1=sum(df1['abs_diff_1'])/sum(df1['abs_diff_2'])\n",
    "    mase2=sum(df2['abs_diff_1'])/sum(df2['abs_diff_2'])\n",
    "    mase3=sum(df3['abs_diff_1'])/sum(df3['abs_diff_2'])\n",
    "    mase4=sum(df4['abs_diff_1'])/sum(df4['abs_diff_2'])\n",
    "    mase5=sum(df5['abs_diff_1'])/sum(df5['abs_diff_2'])\n",
    "    masedf = pd.DataFrame({'dp_range' : ['8_14','15_21','22_28','29_32'],\n",
    "                           'MASE':[mase2, mase3, mase4, mase5]})\n",
    "    #MAPE\n",
    "    #newdf1=df1[df1['OTB_to_survive']!=0]\n",
    "    newdf2=df2[df2['OTB_to_survive']!=0]\n",
    "    newdf3=df3[df3['OTB_to_survive']!=0]\n",
    "    newdf4=df4[df4['OTB_to_survive']!=0]\n",
    "    newdf5=df5[df5['OTB_to_survive']!=0]\n",
    "    #mape1=mean_absolute_percentage_error(newdf1['OTB_to_survive'], newdf1['fcst_svv'])\n",
    "    mape2=mean_absolute_percentage_error(newdf2['OTB_to_survive'], newdf2['fcst_svv'])\n",
    "    mape3=mean_absolute_percentage_error(newdf3['OTB_to_survive'], newdf3['fcst_svv']) \n",
    "    mape4=mean_absolute_percentage_error(newdf4['OTB_to_survive'], newdf4['fcst_svv'])\n",
    "    mape5=mean_absolute_percentage_error(newdf5['OTB_to_survive'], newdf5['fcst_svv'])\n",
    "    mapedf = pd.DataFrame({'dp_range' : ['8_14','15_21','22_28','29_32'],\n",
    "                           'MAPE':[mape2, mape3, mape4, mape5]})\n",
    "    \n",
    "    report = pd.merge(pd.merge(maedf, mapedf, on='dp_range'), masedf, on='dp_range')\n",
    "    report = report[['dp_range','MAE','MAPE','MASE']]\n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get MAE, MAPE, MASE\n",
    "def eval(df):\n",
    "    df['fcst_svv']=df['OTB']-df['OTB']*df['predict_cxl_rate']\n",
    "    df=df[df['days_prior']!=0]\n",
    "    avg_svv_rt = 0.9239133\n",
    "    df['abs_diff_1']=abs(df['OTB_to_survive']-df['fcst_svv'])\n",
    "    df['abs_diff_2']=abs(df['OTB_to_survive']-avg_svv_rt*df['OTB'])\n",
    "    df1=df[df['dp_range']=='1_7']\n",
    "    df2=df[df['dp_range']=='8_14']\n",
    "    df3=df[df['dp_range']=='15_21']\n",
    "    df4=df[df['dp_range']=='22_28']\n",
    "    df5=df[df['dp_range']=='29_32']\n",
    "    # MAE\n",
    "    mae1=mean_absolute_error(df1['OTB_to_survive'], df1['fcst_svv'])\n",
    "    mae2=mean_absolute_error(df2['OTB_to_survive'], df2['fcst_svv'])\n",
    "    mae3=mean_absolute_error(df3['OTB_to_survive'], df3['fcst_svv'])\n",
    "    mae4=mean_absolute_error(df4['OTB_to_survive'], df4['fcst_svv'])\n",
    "    mae5=mean_absolute_error(df5['OTB_to_survive'], df5['fcst_svv'])\n",
    "    maedf = pd.DataFrame({'dp_range' : ['1_7','8_14','15_21','22_28','29_32'],\n",
    "                           'MAE':[mae1, mae2, mae3, mae4, mae5]})\n",
    "    #MASE\n",
    "    mase1=sum(df1['abs_diff_1'])/sum(df1['abs_diff_2'])\n",
    "    mase2=sum(df2['abs_diff_1'])/sum(df2['abs_diff_2'])\n",
    "    mase3=sum(df3['abs_diff_1'])/sum(df3['abs_diff_2'])\n",
    "    mase4=sum(df4['abs_diff_1'])/sum(df4['abs_diff_2'])\n",
    "    mase5=sum(df5['abs_diff_1'])/sum(df5['abs_diff_2'])\n",
    "    masedf = pd.DataFrame({'dp_range' : ['1_7','8_14','15_21','22_28','29_32'],\n",
    "                           'MASE':[mase1, mase2, mase3, mase4, mase5]})\n",
    "    #MAPE\n",
    "    newdf1=df1[df1['OTB_to_survive']!=0]\n",
    "    newdf2=df2[df2['OTB_to_survive']!=0]\n",
    "    newdf3=df3[df3['OTB_to_survive']!=0]\n",
    "    newdf4=df4[df4['OTB_to_survive']!=0]\n",
    "    newdf5=df5[df5['OTB_to_survive']!=0]\n",
    "    mape1=mean_absolute_percentage_error(newdf1['OTB_to_survive'], newdf1['fcst_svv'])\n",
    "    mape2=mean_absolute_percentage_error(newdf2['OTB_to_survive'], newdf2['fcst_svv'])\n",
    "    mape3=mean_absolute_percentage_error(newdf3['OTB_to_survive'], newdf3['fcst_svv']) \n",
    "    mape4=mean_absolute_percentage_error(newdf4['OTB_to_survive'], newdf4['fcst_svv'])\n",
    "    mape5=mean_absolute_percentage_error(newdf5['OTB_to_survive'], newdf5['fcst_svv'])\n",
    "    mapedf = pd.DataFrame({'dp_range' : ['1_7','8_14','15_21','22_28','29_32'],\n",
    "                           'MAPE':[mape1, mape2, mape3, mape4, mape5]})\n",
    "    \n",
    "    report = pd.merge(pd.merge(maedf, mapedf, on='dp_range'), masedf, on='dp_range')\n",
    "    report = report[['dp_range','MAE','MAPE','MASE']]\n",
    "    return report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atlanta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1_train = pd.read_csv(\"h1_train.csv\", sep=',',header=0, index_col=0)\n",
    "h1_test = pd.read_csv(\"h1_test.csv\", sep=',',header=0, index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adding this, the result seems to be worse..so ignore this cell.\n",
    "\n",
    "# Add lag 1,2,3,4,5,6,7 for cumulative bookings\n",
    "\n",
    "#h1_train = h1_train.sort_values(['stay_dt','product_type','days_prior'])\n",
    "#h1_train['lag1'] = h1_train.sort_values(['stay_dt','product_type','days_prior']).groupby(['stay_dt','product_type'])['cummulative_gross_bookings'].shift(1)\n",
    "#h1_train['lag2'] = h1_train.sort_values(['stay_dt','product_type','days_prior']).groupby(['stay_dt','product_type'])['cummulative_gross_bookings'].shift(2)\n",
    "#h1_train['lag3'] = h1_train.sort_values(['stay_dt','product_type','days_prior']).groupby(['stay_dt','product_type'])['cummulative_gross_bookings'].shift(3)\n",
    "#h1_train['lag4'] = h1_train.sort_values(['stay_dt','product_type','days_prior']).groupby(['stay_dt','product_type'])['cummulative_gross_bookings'].shift(4)\n",
    "#h1_train['lag5'] = h1_train.sort_values(['stay_dt','product_type','days_prior']).groupby(['stay_dt','product_type'])['cummulative_gross_bookings'].shift(5)\n",
    "#h1_train['lag6'] = h1_train.sort_values(['stay_dt','product_type','days_prior']).groupby(['stay_dt','product_type'])['cummulative_gross_bookings'].shift(6)\n",
    "#h1_train['lag7'] = h1_train.sort_values(['stay_dt','product_type','days_prior']).groupby(['stay_dt','product_type'])['cummulative_gross_bookings'].shift(7)\n",
    "\n",
    "#h1_test = h1_test.sort_values(['stay_dt','product_type','days_prior'])\n",
    "#h1_test['lag1'] = h1_test.sort_values(['stay_dt','product_type','days_prior']).groupby(['stay_dt','product_type'])['cummulative_gross_bookings'].shift(1)\n",
    "#h1_test['lag2'] = h1_test.sort_values(['stay_dt','product_type','days_prior']).groupby(['stay_dt','product_type'])['cummulative_gross_bookings'].shift(2)\n",
    "#h1_test['lag3'] = h1_test.sort_values(['stay_dt','product_type','days_prior']).groupby(['stay_dt','product_type'])['cummulative_gross_bookings'].shift(3)\n",
    "#h1_test['lag4'] = h1_test.sort_values(['stay_dt','product_type','days_prior']).groupby(['stay_dt','product_type'])['cummulative_gross_bookings'].shift(4)\n",
    "#h1_test['lag5'] = h1_test.sort_values(['stay_dt','product_type','days_prior']).groupby(['stay_dt','product_type'])['cummulative_gross_bookings'].shift(5)\n",
    "#h1_test['lag6'] = h1_test.sort_values(['stay_dt','product_type','days_prior']).groupby(['stay_dt','product_type'])['cummulative_gross_bookings'].shift(6)\n",
    "#h1_test['lag7'] = h1_test.sort_values(['stay_dt','product_type','days_prior']).groupby(['stay_dt','product_type'])['cummulative_gross_bookings'].shift(7)\n",
    "\n",
    "#h1_train=h1_train.dropna()\n",
    "#h1_test=h1_test.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in-sample\n",
      "  dp_range       MAE      MAPE      MASE\n",
      "0      1_7  2.368022  0.064146  0.877747\n",
      "1     8_14  1.881936  0.080786  0.945171\n",
      "2    15_21  2.166557  0.123801  1.037125\n",
      "3    22_28  1.906555  0.133057  1.031771\n",
      "4    29_32  1.627566  0.123220  0.888011\n",
      "on test data\n",
      "  dp_range       MAE      MAPE      MASE\n",
      "0      1_7  2.319101  0.058695  0.871561\n",
      "1     8_14  1.777058  0.071543  0.886217\n",
      "2    15_21  1.806328  0.121477  0.918217\n",
      "3    22_28  1.757365  0.129115  0.984496\n",
      "4    29_32  1.585570  0.110684  0.988561\n"
     ]
    }
   ],
   "source": [
    "X_train = h1_train[['days_prior', 'day_type', 'OP_FENC',\n",
    "       'CORP_BUS', 'MEM_OTHER', 'GOV_UNFENC', 'GROUP', 'price',\n",
    "       'cummulative_gross_bookings']]\n",
    "X_test = h1_test[['days_prior', 'day_type', 'OP_FENC',\n",
    "       'CORP_BUS', 'MEM_OTHER', 'GOV_UNFENC', 'GROUP', 'price',\n",
    "       'cummulative_gross_bookings']]\n",
    "y_train = h1_train['OTB_cxl_rate']\n",
    "y_test = h1_test['OTB_cxl_rate']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scl = scaler.transform(X_train)\n",
    "X_test_scl = scaler.transform(X_test)\n",
    "\n",
    "mlp = MLPRegressor(hidden_layer_sizes=(4,4,4,4),random_state=22, max_iter=1000,solver='lbfgs',activation='tanh')  \n",
    "mlp.fit(X_train_scl, y_train)\n",
    "\n",
    "h1_train['predict_cxl_rate']=list(mlp.predict(X_train_scl))\n",
    "h1_test['predict_cxl_rate']=list(mlp.predict(X_test_scl))  \n",
    "\n",
    "print('in-sample')\n",
    "print(eval(h1_train))\n",
    "print('on test data')\n",
    "print(eval(h1_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in-sample\n",
      "  dp_range       MAE      MAPE      MASE\n",
      "0      1_7  1.979326  0.063856  0.733670\n",
      "1     8_14  1.894641  0.086895  0.951552\n",
      "2    15_21  2.066572  0.135174  0.989262\n",
      "3    22_28  1.848474  0.142391  1.000339\n",
      "4    29_32  1.642377  0.140447  0.896091\n",
      "on test data\n",
      "  dp_range       MAE      MAPE      MASE\n",
      "0      1_7  1.795387  0.049621  0.674739\n",
      "1     8_14  1.879398  0.071869  0.937254\n",
      "2    15_21  2.327889  0.129492  1.183344\n",
      "3    22_28  2.286086  0.133826  1.280691\n",
      "4    29_32  1.811799  0.116129  1.129608\n"
     ]
    }
   ],
   "source": [
    "h1_train['naive_cxl_rate']=sum(h1_train['OTB_to_be_cxl'])/sum(h1_train['OTB'])\n",
    "h1_test['naive_cxl_rate']=h1_train['naive_cxl_rate'][1]\n",
    "\n",
    "X_train = h1_train[['days_prior', 'day_type', 'OP_FENC',\n",
    "       'CORP_BUS', 'MEM_OTHER', 'GOV_UNFENC', 'GROUP', 'price',\n",
    "       'cummulative_gross_bookings','naive_cxl_rate']]\n",
    "X_test = h1_test[['days_prior', 'day_type', 'OP_FENC',\n",
    "       'CORP_BUS', 'MEM_OTHER', 'GOV_UNFENC', 'GROUP', 'price',\n",
    "       'cummulative_gross_bookings','naive_cxl_rate']]\n",
    "y_train = h1_train['OTB_cxl_rate']\n",
    "y_test = h1_test['OTB_cxl_rate']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scl = scaler.transform(X_train)\n",
    "X_test_scl = scaler.transform(X_test)\n",
    "\n",
    "mlp = MLPRegressor(hidden_layer_sizes=(3,8,10,10,6),random_state=666, max_iter=1000,solver='lbfgs',activation='tanh')  \n",
    "mlp.fit(X_train_scl, y_train)\n",
    "\n",
    "h1_train['predict_cxl_rate']=list(mlp.predict(X_train_scl))\n",
    "h1_test['predict_cxl_rate']=list(mlp.predict(X_test_scl))  \n",
    "\n",
    "print('in-sample')\n",
    "print(eval(h1_train))\n",
    "print('on test data')\n",
    "print(eval(h1_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it seems adding constant column is not helping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New York"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in-sample\n",
      "  dp_range       MAE      MAPE      MASE\n",
      "0      1_7  2.384580  0.050527  0.529282\n",
      "1     8_14  2.512771  0.065594  0.763107\n",
      "2    15_21  2.505389  0.072957  0.749727\n",
      "3    22_28  2.446710  0.089560  0.770875\n",
      "4    29_32  2.592294  0.108154  0.843728\n",
      "on test data\n",
      "  dp_range       MAE      MAPE      MASE\n",
      "0      1_7  2.421076  0.041146  0.424182\n",
      "1     8_14  2.716663  0.051084  0.645697\n",
      "2    15_21  2.910460  0.061688  0.774370\n",
      "3    22_28  2.754595  0.079671  0.768076\n",
      "4    29_32  2.964800  0.090708  0.771685\n"
     ]
    }
   ],
   "source": [
    "h2_train = pd.read_csv(\"h2_train.csv\", sep=',',header=0, index_col=0)\n",
    "h2_test = pd.read_csv(\"h2_test.csv\", sep=',',header=0, index_col=0)\n",
    "\n",
    "X_train = h2_train[['days_prior', 'day_type', 'OP_FENC_OTH','CORP_GROUP_TACT', 'UNF_WHOLE_MEM_BUS', 'GOVERNMENT', 'price',\n",
    "       'cummulative_gross_bookings']]\n",
    "X_test = h2_test[['days_prior', 'day_type', 'OP_FENC_OTH','CORP_GROUP_TACT', 'UNF_WHOLE_MEM_BUS', 'GOVERNMENT', 'price',\n",
    "       'cummulative_gross_bookings']]\n",
    "y_train = h2_train['OTB_cxl_rate']\n",
    "y_test = h2_test['OTB_cxl_rate']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scl = scaler.transform(X_train)\n",
    "X_test_scl = scaler.transform(X_test)\n",
    "\n",
    "mlp = MLPRegressor(hidden_layer_sizes=(5,10,2,4), max_iter=1000, random_state=333)  \n",
    "mlp.fit(X_train_scl, y_train)\n",
    "\n",
    "h2_train['predict_cxl_rate']=list(mlp.predict(X_train_scl))\n",
    "h2_test['predict_cxl_rate']=list(mlp.predict(X_test_scl))  \n",
    "\n",
    "print('in-sample')\n",
    "print(eval(h2_train))\n",
    "print('on test data')\n",
    "print(eval(h2_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add constant column and lag to the data\n",
    "h2_train = pd.read_csv(\"h2_train.csv\", sep=',',header=0, index_col=0)\n",
    "h2_test = pd.read_csv(\"h2_test.csv\", sep=',',header=0, index_col=0)\n",
    "\n",
    "## Adding this, the result seems to be worse..\n",
    "\n",
    "# Add naive cxl rate as the constant column\n",
    "h2_train['naive_cxl_rate']=sum(h2_train['OTB_to_be_cxl'])/sum(h2_train['OTB'])\n",
    "h2_test['naive_cxl_rate']=h2_train['naive_cxl_rate'][1]\n",
    "\n",
    "# Add lag 1,2,3,4,5,6,7 for cumulative bookings\n",
    "\n",
    "h2_train = h2_train.sort_values(['stay_dt','product_type','days_prior'])\n",
    "h2_train['lag1'] = h2_train.sort_values(['stay_dt','product_type','days_prior']).groupby(['stay_dt','product_type'])['cummulative_gross_bookings'].shift(1)\n",
    "h2_train['lag2'] = h2_train.sort_values(['stay_dt','product_type','days_prior']).groupby(['stay_dt','product_type'])['cummulative_gross_bookings'].shift(2)\n",
    "h2_train['lag3'] = h2_train.sort_values(['stay_dt','product_type','days_prior']).groupby(['stay_dt','product_type'])['cummulative_gross_bookings'].shift(3)\n",
    "h2_train['lag4'] = h2_train.sort_values(['stay_dt','product_type','days_prior']).groupby(['stay_dt','product_type'])['cummulative_gross_bookings'].shift(4)\n",
    "h2_train['lag5'] = h2_train.sort_values(['stay_dt','product_type','days_prior']).groupby(['stay_dt','product_type'])['cummulative_gross_bookings'].shift(5)\n",
    "h2_train['lag6'] = h2_train.sort_values(['stay_dt','product_type','days_prior']).groupby(['stay_dt','product_type'])['cummulative_gross_bookings'].shift(6)\n",
    "h2_train['lag7'] = h2_train.sort_values(['stay_dt','product_type','days_prior']).groupby(['stay_dt','product_type'])['cummulative_gross_bookings'].shift(7)\n",
    "\n",
    "h2_test = h2_test.sort_values(['stay_dt','product_type','days_prior'])\n",
    "h2_test['lag1'] = h2_test.sort_values(['stay_dt','product_type','days_prior']).groupby(['stay_dt','product_type'])['cummulative_gross_bookings'].shift(1)\n",
    "h2_test['lag2'] = h2_test.sort_values(['stay_dt','product_type','days_prior']).groupby(['stay_dt','product_type'])['cummulative_gross_bookings'].shift(2)\n",
    "h2_test['lag3'] = h2_test.sort_values(['stay_dt','product_type','days_prior']).groupby(['stay_dt','product_type'])['cummulative_gross_bookings'].shift(3)\n",
    "h2_test['lag4'] = h2_test.sort_values(['stay_dt','product_type','days_prior']).groupby(['stay_dt','product_type'])['cummulative_gross_bookings'].shift(4)\n",
    "h2_test['lag5'] = h2_test.sort_values(['stay_dt','product_type','days_prior']).groupby(['stay_dt','product_type'])['cummulative_gross_bookings'].shift(5)\n",
    "h2_test['lag6'] = h2_test.sort_values(['stay_dt','product_type','days_prior']).groupby(['stay_dt','product_type'])['cummulative_gross_bookings'].shift(6)\n",
    "h2_test['lag7'] = h2_test.sort_values(['stay_dt','product_type','days_prior']).groupby(['stay_dt','product_type'])['cummulative_gross_bookings'].shift(7)\n",
    "\n",
    "h2_train=h2_train.dropna()\n",
    "h2_test=h2_test.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in-sample\n",
      "  dp_range       MAE      MAPE      MASE\n",
      "0     8_14  2.339891  0.065075  0.708774\n",
      "1    15_21  2.363535  0.070687  0.707278\n",
      "2    22_28  2.143113  0.085499  0.675222\n",
      "3    29_32  2.045686  0.103470  0.665821\n",
      "on test data\n",
      "  dp_range       MAE      MAPE      MASE\n",
      "0     8_14  3.102472  0.053058  0.733287\n",
      "1    15_21  2.635337  0.058974  0.701170\n",
      "2    22_28  2.596872  0.071776  0.724098\n",
      "3    29_32  2.638321  0.082227  0.686708\n"
     ]
    }
   ],
   "source": [
    "X_train = h2_train[['days_prior', 'day_type', 'OP_FENC_OTH','CORP_GROUP_TACT', 'UNF_WHOLE_MEM_BUS', 'GOVERNMENT', 'price',\n",
    "       'cummulative_gross_bookings','naive_cxl_rate', 'lag1','lag2','lag3','lag4','lag5','lag6','lag7']]\n",
    "X_test = h2_test[['days_prior', 'day_type', 'OP_FENC_OTH','CORP_GROUP_TACT', 'UNF_WHOLE_MEM_BUS', 'GOVERNMENT', 'price',\n",
    "       'cummulative_gross_bookings','naive_cxl_rate', 'lag1','lag2','lag3','lag4','lag5','lag6','lag7']]\n",
    "y_train = h2_train['OTB_cxl_rate']\n",
    "y_test = h2_test['OTB_cxl_rate']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scl = scaler.transform(X_train)\n",
    "X_test_scl = scaler.transform(X_test)\n",
    "\n",
    "mlp = MLPRegressor(hidden_layer_sizes=(3,4,5,6), max_iter=1000, random_state=333, activation='relu', solver='adam')\n",
    "mlp.fit(X_train_scl, y_train)\n",
    "\n",
    "h2_train['predict_cxl_rate']=list(mlp.predict(X_train_scl))\n",
    "h2_test['predict_cxl_rate']=list(mlp.predict(X_test_scl))  \n",
    "\n",
    "print('in-sample')\n",
    "print(eval2(h2_train))\n",
    "print('on test data')\n",
    "print(eval2(h2_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it seems adding the constant and lag columns are not helping..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
